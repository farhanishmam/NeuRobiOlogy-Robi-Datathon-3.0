# -*- coding: utf-8 -*-
"""Robi Datathon P3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1snCb3CFutvqRCwVud7SLifcm6uoRJgZC
"""

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
from sklearn.ensemble import RandomForestClassifier

!unzip problem3.zip

purchases = pd.read_csv('purchase.csv')
boxes = pd.read_csv('boxes.csv')
test = pd.read_csv('problem 3.csv')

test.shape

purchases.loc[:, 'PURCHASE_DATE'] = pd.to_datetime(purchases['PURCHASE_DATE'], format='%d/%m/%Y')
purchases['PURCHASE_DATE'] = pd.to_datetime(purchases['PURCHASE_DATE'])

dsAll = pd.merge(purchases, boxes, on='BOX_ID', how='inner')
dsTestAll = pd.merge(test, dsAll, on='MAGIC_KEY', how='left')
dsTest = dsTestAll

dsTest.describe()

z_scores = (dsTest['BOX_COUNT'] - dsTest['BOX_COUNT'].mean()) / dsTest['BOX_COUNT'].std()

# Define a threshold for identifying outliers (e.g., Z-score greater than 3 or less than -3)
threshold = 3

# Find the outliers using the Z-score
outliers = dsTest[abs(z_scores) > threshold]
outliers

dsTest[dsTest['BOX_COUNT']>15]

dsTest = dsTest.drop(index=451)
dsTest[dsTest['BOX_COUNT']>15]

dsTest['MEAT'] = dsTest['MEAT']*dsTest['BOX_COUNT']
dsTest['MILK'] = dsTest['MILK']*dsTest['BOX_COUNT']
dsTest['UNIT_PRICE'] = dsTest['UNIT_PRICE']*dsTest['BOX_COUNT']
dsTest = dsTest.drop(['BOX_ID', 'BOX_COUNT'], axis=1)
dsTest

dsTest = dsTest.drop(['QUALITY', 'DELIVERY_OPTION'], axis=1)

data2=dsTest

dc1 = data2[(data2['PURCHASE_DATE'] >= '2019-01-01') & (data2['PURCHASE_DATE'] <= '2019-01-15')]
dc2 = data2[(data2['PURCHASE_DATE'] >= '2019-01-16') & (data2['PURCHASE_DATE'] <= '2019-01-31')]
dc3 = data2[(data2['PURCHASE_DATE'] >= '2019-02-01') & (data2['PURCHASE_DATE'] <= '2019-02-14')]
dc4 = data2[(data2['PURCHASE_DATE'] >= '2019-02-15') & (data2['PURCHASE_DATE'] <= '2019-02-28')]

dc1_grouped = dc1.groupby('MAGIC_KEY').agg({'MEAT': 'sum', 'PURCHASE_DATE': 'count'})
dc1_grouped.rename(columns={'MEAT': 'M1', 'PURCHASE_DATE': 'C1'}, inplace=True)

dc2_grouped = dc2.groupby('MAGIC_KEY').agg({'MEAT': 'sum', 'PURCHASE_DATE': 'count'})
dc2_grouped.rename(columns={'MEAT': 'M2', 'PURCHASE_DATE': 'C2'}, inplace=True)

dc3_grouped = dc3.groupby('MAGIC_KEY').agg({'MEAT': 'sum', 'PURCHASE_DATE': 'count'})
dc3_grouped.rename(columns={'MEAT': 'M3', 'PURCHASE_DATE': 'C3'}, inplace=True)

dc4_grouped = dc4.groupby('MAGIC_KEY').agg({'MEAT': 'sum', 'PURCHASE_DATE': 'count'})
dc4_grouped.rename(columns={'MEAT': 'M4', 'PURCHASE_DATE': 'C4'}, inplace=True)

pd.merge(data2, dc1_grouped, on = 'MAGIC_KEY', how = 'inner')

data2

data3 = data2.copy()
data3['LASTMEAT'] = data3['MEAT']

summary_table = data3.groupby('MAGIC_KEY').agg({
    'PURCHASE_DATE': 'max',  # Last purchase date
    'LASTMEAT': 'last',           # Meat in last purchase
    'MEAT': 'mean',
    'MILK': 'mean',
    'UNIT_PRICE': 'mean'      # Average unit price
})

reference_date = pd.to_datetime('2019-03-01')
summary_table
summary_table['delD'] = (reference_date - summary_table['PURCHASE_DATE']).dt.days

summary_table

summary_table.drop(columns=['PURCHASE_DATE'], inplace=True)

dc1

summary_table.reset_index()

x = pd.merge(summary_table.reset_index(), dc1_grouped.reset_index(), on ='MAGIC_KEY', how = 'left')
x = pd.merge(x, dc2_grouped.reset_index(), on =['MAGIC_KEY'], how = 'left')
x = pd.merge(x, dc3_grouped.reset_index(), on =['MAGIC_KEY'], how = 'left')
x = pd.merge(x, dc4_grouped.reset_index(), on =['MAGIC_KEY'], how = 'left')
x = x.fillna(0)

x['LASTMEAT'].describe()

xTrain = np.array(x[['LASTMEAT','MEAT','MILK','UNIT_PRICE',	'delD',	'M1',	'M2','M3']])
yTrain = np.array(x[['M4']]).ravel()



from sklearn.preprocessing import StandardScaler

# Assuming 'X' is your DataFrame containing the input features

# Initialize the StandardScaler
scaler = StandardScaler()

# Fit the scaler to the data and transform the data
X_scaled = scaler.fit_transform(xTrain)

scaler = StandardScaler()
y_scaled = scaler.fit_transform(yTrain)

from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import accuracy_score
from sklearn.metrics import mean_squared_error
import pandas as pd

rf_regressor = RandomForestRegressor(n_estimators=100, random_state=42)
# Train the regressor
rf_regressor.fit(X_scaled, yTrain)

# Make predictions on the test set
y_pred = rf_regressor.predict(X_scaled)

# Calculate the mean squared error
mse = mean_squared_error(yTrain, y_pred)
print("Mean Squared Error:", mse)

from sklearn.ensemble import GradientBoostingRegressor
gb_regressor = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, random_state=42)

# Train the regressor
gb_regressor.fit(xTrain, yTrain)

# Make predictions on the test set
y_pred = gb_regressor.predict(xTrain)

# Calculate the mean squared error
mse = mean_squared_error(yTrain, y_pred)
print("Mean Squared Error:", mse)

from sklearn.ensemble import AdaBoostRegressor
adaboost_regressor = AdaBoostRegressor(n_estimators=100, random_state=42)

# Train the regressor
adaboost_regressor.fit(xTrain, yTrain)

# Make predictions on the test set
y_pred = adaboost_regressor.predict(xTrain)

# Calculate mean squared error
mse = mean_squared_error(yTrain, y_pred)

import tensorflow as tf
# Define the model architecture

class MSECallback(tf.keras.callbacks.Callback):
    def on_epoch_end(self, epoch, logs=None):
        mse = logs.get('val_loss')  # Get the MSE from validation loss
        print(f"Epoch {epoch+1} - Validation MSE: {mse:.4f}")


model = tf.keras.Sequential([
    tf.keras.layers.Dense(64, activation='relu', input_shape=(xTrain.shape[1],)),
    tf.keras.layers.Dense(32, activation='relu'),
    tf.keras.layers.Dense(32, activation='relu'),
    tf.keras.layers.Dense(32, activation='relu'),
    tf.keras.layers.Dense(1)  # Output layer with 1 neuron for regression
])

# Compile the model
model.compile(optimizer='adam', loss='mean_squared_error')
mse_callback = MSECallback()
# Train the model
model.fit(xTrain, yTrain, epochs=10, batch_size=32, validation_split=0.15, callbacks=[mse_callback])

from sklearn.neighbors import KNeighborsRegressor
from sklearn.metrics import mean_squared_error

# Initialize KNeighborsRegressor
knn_regressor = KNeighborsRegressor(n_neighbors=5)  # You can adjust the number of neighbors as needed

# Train the regressor
knn_regressor.fit(xTrain, yTrain)

# Make predictions on the test set
y_pred = knn_regressor.predict(xTrain)

# Calculate mean squared error
mse = mean_squared_error(yTrain, y_pred)
print("Mean Squared Error:", mse)

xTest = np.array(x[['LASTMEAT','MEAT','MILK','UNIT_PRICE',	'delD',	'M1',	'M2','M3']])

xTestDF = x[['MAGIC_KEY','LASTMEAT','MEAT','MILK','UNIT_PRICE',	'delD',	'M2','M3'	,'M4'	]]
xTestDF = pd.merge(test,xTestDF, on = 'MAGIC_KEY', how = 'left')
xTestDF

y_pred = gb_regressor.predict(xTest)

xTest = np.array(xTestDF[['LASTMEAT','MEAT','MILK','UNIT_PRICE',	'delD',	'M2',	'M3'	,'M4']])

y_pred = rf_regressor.predict(xTest)

test['MEAT'] = y_pred

test.to_csv('p3F2.csv', index=False)

y_pred.shape

dsTest_feb = dsTest[(dsTest['PURCHASE_DATE'].dt.month == 2)]
dsTest_feb

dsTestX = dsTest

dsTestX[(dsTestX['PURCHASE_DATE'] >= '2019-02-14') & (dsTestX['PURCHASE_DATE'] <= '2019-02-28')]

purchase_counts = dsTestX['PURCHASE_DATE'].value_counts().sort_index()

# Plot the number of purchases for each purchase date
purchase_counts.plot(figsize=(10, 6))

first_purchase_dates = dsTestX.groupby('MAGIC_KEY')['PURCHASE_DATE'].min()
last_purchase_dates = dsTestX.groupby('MAGIC_KEY')['PURCHASE_DATE'].max()

# Merge the aggregated results back into the original DataFrame
dsTestX2 = dsTestX.merge(first_purchase_dates, on='MAGIC_KEY', suffixes=('', '_first'))
dsTestX2 = dsTestX2.merge(last_purchase_dates, on='MAGIC_KEY', suffixes=('', '_last'))
dsTestX2

dsTest_last_purchase = dsTestX2[dsTestX2['PURCHASE_DATE'] == dsTestX2['PURCHASE_DATE_last']]
dsTest_last_purchase

columns_to_drop = ['PURCHASE_DATE_last']
dsTest_last_purchase.drop(columns=columns_to_drop, inplace=True)

ds2 = dsTest_last_purchase.groupby(['MAGIC_KEY', 'PURCHASE_DATE', 'PURCHASE_DATE_first'])['MEAT'].sum().reset_index()
ds2

start_date = pd.to_datetime('2019-02-14')
end_date = pd.to_datetime('2019-02-28')
end_date2 = pd.to_datetime('2019-03-01')
ds2.loc[(ds2['PURCHASE_DATE_first'] >= start_date) & (ds2['PURCHASE_DATE_first'] <= end_date), 'DUR'] = (end_date2 - ds2['PURCHASE_DATE_first']).dt.days

# Set DUR to 15 for other rows
ds2['DUR'].fillna(15, inplace=True)
ds2

ds2

filtered_dsTest = dsTest[(dsTest['PURCHASE_DATE'] >= '2019-02-14') &
                         (dsTest['PURCHASE_DATE'] <= '2019-02-28')]

# Group by 'MAGIC_KEY' and calculate the sum of 'MEAT' for each group
total_meat_purchased = filtered_dsTest.groupby('MAGIC_KEY')['MEAT'].sum()
tm = total_meat_purchased.reset_index()
tm.rename(columns={'MEAT': 'MEAT15'}, inplace=True)
tm

ds3 = pd.merge(ds2, tm, on = 'MAGIC_KEY', how = 'left')
ds3['MEAT15'] = ds3['MEAT15'].fillna(0)
ds3

def calculate_Y(row):
    if row['PURCHASE_DATE'].strftime('%Y-%m-%d') >= '2019-02-14' and row['PURCHASE_DATE'].strftime('%Y-%m-%d') <= '2019-02-28':
        return row['MEAT15']
    else:
        return row['MEAT']

test.shape

new_df = pd.DataFrame({'MAGIC_KEY': ds3['MAGIC_KEY'], 'Y': ds3.apply(calculate_Y, axis=1)})
pred = test.merge(new_df, on = 'MAGIC_KEY', how = 'left')

mean_Y = pred['Y'].mean()

# Replace null values with the mean value
pred['Y'].fillna(mean_Y, inplace=True)

pred = pred.rename(columns={'Y': 'MEAT'})

pred.to_csv('p3vF.csv', index=False)

pred.merge(dsTestAll, on = 'MAGIC_KEY', how = 'inner' )

pd.merge(dsTest, p, on = 'MAGIC_KEY', how = 'inner')

# Group by customer ID and calculate the number of days since the last purchase
dsTestS = dsTest.sort_values(by=['MAGIC_KEY', 'PURCHASE_DATE'])
#dsTestS['PURCHASE_DATE'] = pd.to_datetime(dsTestS['PURCHASE_DATE'])
dsTestS['delDays'] = dsTestS.groupby('MAGIC_KEY')['PURCHASE_DATE'].diff().dt.days
# Fill NaN values (for the first purchase of each customer) with 0
#dsTestS['delDays'] = dsTestS['delDays'].fillna(0)

dsTestS

dsTestS['QUALITY'].value_counts()

dsTestS['DELIVERY_OPTION'].value_counts()

dsTestS=dsTestS.dropna()

average_delDays = dsTestS.groupby('MAGIC_KEY')['delDays'].mean()
average_delDays

dsTest15 = dsTest[(dsTest['PURCHASE_DATE'] >= '2019-02-14') & (dsTest['PURCHASE_DATE'] <= '2019-02-28')]

dsTestS = dsTest.sort_values(by=['MAGIC_KEY', 'PURCHASE_DATE'])
dsTestS['delDays'] = dsTestS.groupby('MAGIC_KEY')['PURCHASE_DATE'].diff().dt.days
# Fill NaN values (for the first purchase of each customer) with 0
dsTestS['delDays'] = dsTestS['delDays'].fillna(0)

dsTestS.head()

dsTestS['delDays'].value_counts().reset_index()['count'][:50].sum()/dsTestS['delDays'].value_counts().reset_index()['count'][:].sum()

dsTestS['delDays'].value_counts().reset_index()

"""Observing data from last 2 months should be enough

## Splitting
"""

dsTestS['delDays'].value_counts().reset_index()

trainDs = dsTestS[(dsTestS['PURCHASE_DATE'] >= '2018-12-01') & (dsTestS['PURCHASE_DATE'] <= '2019-01-31')]
testDs = dsTestS[(dsTestS['PURCHASE_DATE'] >= '2019-02-01') & (dsTestS['PURCHASE_DATE'] <= '2019-02-15')]

trainDs

"""# Analysis"""

trainDs.groupby(['MAGIC_KEY', 'BOX_ID']).size().reset_index(name='PurchaseCount')['PurchaseCount'].value_counts()

trainDs['QUALITY'].value_counts()

trainDs['DELIVERY_OPTION'].value_counts()

trainDs['BOX_ID'].value_counts()

"""## Digital Payment can be ignored"""

trainDs['BOX_COUNT'].value_counts()

from sklearn.metrics import mean_squared_error

def eval(pred):
  testMeat = testDs.groupby('MAGIC_KEY')['MEAT'].sum()
  #predict = pd.merge(testMeat['MAGIC_KEY'], pred, on = 'MAGIC_KEY', how = 'left')
  mse = mean_squared_error(testMeat,pred)
  return np.sqrt(mse)

